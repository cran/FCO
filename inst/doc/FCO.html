<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>FCO</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">FCO</h1>



<pre><code>## Loading required package: lavaan</code></pre>
<pre><code>## This is lavaan 0.6-20
## lavaan is FREE software! Please report any bugs.</code></pre>
<pre><code>## Loading required package: FCO</code></pre>
<pre><code>## Loading required package: dplyr</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<div id="overview" class="section level1">
<h1>Overview</h1>
<div id="motivation" class="section level2">
<h2>Motivation</h2>
<p>This package allows users to derive flexible cutoffs for the
evaluation of absolute and relative model fit in Covariance-Based
Structural Equation Modeling (CBSEM).</p>
<p>For CBSEM, a multitude of fit indices have been proposed, roughly
classified as Goodness-of-Fit (GoF), such as CFI (Comparative Fit
Index), or Badness-of-Fit (BoF), including SRMR (Standardized Root Mean
Square Residual). Despite its initial appeal, all fit indices are
distorted by characteristics of the model (e.g., size of the model) and
sample characteristics (e.g., sample size) to some degree. Hence, their
ability to determine whether a model has a truly “good” fit or not is
often masked by an unwanted sensitivity to model and sample
characteristics.</p>
<p>Consequentially, widely acknowledged recommendations and guidelines
(e.g., CFI &gt;= .95 indicates “good” fit) struggle with these
distortions because their “fixed” cutoffs remain constant, irrespective
of the model and sample characteristics investigated. For example, CFI
tends to be sensitive to sample size. A rather small sample (e.g., N =
200) will almost always produce smaller CFIs than a rather large sample
(e.g., N = 1,000). Hence, fixed cutoffs for CFI, such as .95 will likely
reject more correct models for small sample sizes and likely accept more
incorrect models for larger sample sizes.</p>
<p>Flexible cutoffs aim to overcome this deficit by providing customized
cutoff values for a given model with specific model and sample
characteristics. For the above example, a flexible cutoff will decrease
(to e.g., .94 for N = 200) or increase (to e.g., .98 for N = 1,000)
depending on the model and its characteristics.</p>
<p>Flexible cutoffs are derived from simulated distributions of
correctly specified Confirmatory Factor Analysis (CFA) models for a wide
range of latent variables (or factors), indicators per latent variable,
sample sizes, factor loadings, and normal as well as non-normal data.
Starting with version 2, some types of SEM are also possible (see
Details). Flexible cutoffs can be understood as the empirical quantile
of a given index for a predefined uncertainty. If an uncertainty of 5
percent (or .05) is accepted a-priori, the 5 percent quantile of the
simulated distribution for correctly specified CFA models, with the
given model and sample characteristics, will determine the flexible
cutoff. Depending on the nature of the underlying fit index, the
appropriate lower (GoF) or upper (BoF) width of the respective
confidence interval, as defined by the quantile, is used to derive the
flexible cutoff.</p>
<p>Thereby, flexible cutoffs are also flexible in the recommendations
they provide. If users are more uncertain about their model, they can
adjust the uncertainty to 10 percent (or .10). When being more certain
about the underlying model, the uncertainty can be adjusted to .1, or
even .001. Note that this uncertainty is inverse to the understanding of
a p-value. A researcher admits how uncertain s/he is about a given model
and thus .10 indicates very conservative cutoffs, while .001 determines
very lenient cutoffs.</p>
<p>Details on the procedure and methods can be found in the paper
“Flexible Cutoff Values for Fit Indices in the Evaluation of Structural
Equation Models” (Niemand and Mai, 2018).</p>
</div>
<div id="version-2.0" class="section level2">
<h2>Version 2.0</h2>
<p>Originally proposed by Niemand &amp; Mai (2018), flexible cutoffs
(hereafter FCO1) have been developed with only a correct model in mind.
This simple, first approach of simulated cutoffs for fit indices hence
cannot consider misspecified models under consideration. To improve on
this, multiple decision rules are introduced that – based on a
misspecified model – allow to determine the Type II error of misfit for
a given cutoff and fit indicator. With this feature in mind, this
enables different decision rules (i.e., what type of error is considered
and how) based on pertinent literature:</p>
<ul>
<li><p>FCO1: The original proposed flexible cutoffs (Niemand &amp; Mai,
2018), only considering Type I error. The p (e.g., 5%) quantile of all
simulated correct models is taken (assuming a GoF like CFI,
alternatively 1-p for a BoF like SRMR).</p></li>
<li><p>FCO2: A modified flexible cutoff considering Type I and II error.
The 1-p (e.g., 95%) quantile of all simulated misfit models is taken
when this quantile is smaller than or equal to the p (e.g., 5%)
quantile. Otherwise, the p (e.g., 5%) quantile of all simulated correct
models is taken. FCO2 (alike FCO1) always provides a cutoff (assuming a
GoF like CFI, alternatively p [misfit model] and 1-p [correct model] for
a BoF like SRMR)</p></li>
<li><p>DFI: A modified dynamic cutoff considering Type I and II error
(McNeish &amp; Wolff, 2023). The 1-p (e.g., 95%) quantile of all
simulated misfit models is taken when this quantile is smaller than or
equal to the p (e.g., 5%) quantile. Otherwise, no cutoff is provided
(NA). The DFI-decision rule tends to provide no cutoff when correct and
misfit models overlap strongly (assuming a GoF like CFI, alternatively p
[misfit model] and 1-p [correct model] for a BoF like SRMR).</p></li>
<li><p>CP: A modified cutoff considering Type I and II error via an
optimal cutpoint (Groskurth et al., 2025). The cutoff is found by taking
the cutoff with the highest sum of sensitivity (1 – Type I error) and 1
– specificity (Type II error) derived from simulated correct and misfit
models via cutpointr::cutpointr. CP always provides a cutoff.</p></li>
<li><p>Fix: Fixed cutoffs (Hu &amp; Bentler, 1999) are also provided for
comparison.</p></li>
</ul>
<p>Since it cannot be objectively determined what level or type of
misspecification (and to which extent) demarcates “acceptable” and
“unacceptable” misfit, generalizability of the misspecification
procedure becomes a vital question. To overcome this issue, a
PROCESS-like (Hayes, 2017) approach is proposed. Instead of expecting
that the user provides an appropriate misspecified model (as in simsem),
which might be highly user-unfriendly, the user only provides a type of
misfit model via model.type. This argument defines the number of
structural model misspecifications (first integer), measurement model
misspecifications (second integer) and residual covariance
misspecifications (third integer) assumed for the misfit model. For
example, “100” refers to one structural model misspecification (factor
correlations set to 0), zero measurement model misspecifications (no
cross-loadings set to 0) and zero residual covariances introduced to the
correct model described in mod. The default is set to “111”, which
corresponds to a model where one correlation, one cross-loading and one
residual covariance may be overlooked. If a researcher is certain, that
only one type of misspecification is important, the value can be changed
to a “100”, “010”, or “001” model for example. Comparing multiple
model.type specifications is recommended.</p>
<p>Based on feedback on the previous versions of FCO, some new features
are integrated, most importantly direct input of a fitted lavaan object
(fit), a one-step calculation of fits, support for different types of
variables, extensive checking of the model and data characteristics by
an internal function providing better warning and error descriptions,
directly setting skewness (sk) and kurtosis (ku), easier parallelization
options and random generation (random) of the misfit model. Further, two
more functions have been introduced to better compare the implications
(e.g., in terms of implied Type I and II errors) across decision rules
and to visualize the simulated correct and misfit model
distributions.</p>
<p>Function flex_co2 now provides detailed information on the quantiles,
cutoffs (all decision rules plus fixed cutoffs), evaluation (including
sum of errors, Type I error, Type II error estimates from the simulation
data), a notation and overlap statistics (percentage overlap from
overlapping::overlap, AUC from cutpointr::cutpointr and a U-test
converted to d from rcompanion::wilcoxonR) to identify the degree of
overlap between simulated correct and misfit model distributions.
Thereby, users can select an appropriate cutoff, inspect which decision
rule works best for their data and model and determine how much the
distributions overlap. Function plot_fit2 complements these features and
plots the simulated distributions for correct and misfit models to
illustrate the distributions, their overlap and the cutoff
quantiles.</p>
<p>Multiple notes are given regarding the estimates 1) “DFI” does not
refer to using the same approach as dynamic::cfaHB as proposed by
McNeish &amp; Wolf (2023). DFI only refers to the authors’ decision
rule, but applying the same PROCESS-like approach as FCO1, FCO2 and CP.
2). For compatibility reasons, functions gen_fit, flex_co, pop_mod,
recommend and recommend_dv are (virtually) unchanged and only apply for
FCO1. Likewise, functions flex_co2 and plot_fit2 only apply for results
of this function gen_fit2.</p>
</div>
<div id="getting-started" class="section level2">
<h2>Getting started</h2>
<div id="installation" class="section level3">
<h3>Installation</h3>
<p>FCO is available on CRAN and can be installed as usual via
install.packages. Then, the package can be loaded, for example with
(required packages, if installed, will be loaded automatically):</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="fu">library</span>(FCO)</span></code></pre></div>
</div>
<div id="basic-usage" class="section level3">
<h3>Basic usage</h3>
<p>In an initial example, we use the data from Holzinger &amp; Swineford
(1939) to illustrate the basic features since version 2.0. The user
postulates the following model and generates a lavaan object from a
cfa.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="fu">library</span>(lavaan)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>HS.model <span class="ot">&lt;-</span> <span class="st">&#39; visual  =~ x1 + x2 + x3</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="st">              textual =~ x4 + x5 + x6</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="st">              speed   =~ x7 + x8 + x9 &#39;</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">cfa</span>(</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>  HS.model,</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>  <span class="at">data =</span> HolzingerSwineford1939</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>)</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a><span class="fu">fitmeasures</span>(fit)</span></code></pre></div>
<p>Based on this fitted object, the user generates simulated fit indices
for two models, a correct model and misspecified model. The correct
model defaults to “NM” (Niemand &amp; Mai, 2018) with equal loadings and
correlations. It is not the empirical model under investigation but a
truly correct model (since one cannot be sure that the model is truly
correct). The misspecified model is generated from a number of
misspecifications that the user perceives as possible for the given
model. In the default case, these are 1 cross-loading, 1 omitted
correlation and 1 residual covariance (111-model).</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co">#Note: Demonstration only! Please use higher numbers of replications for your applications (&gt;= 500).</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>fits <span class="ot">&lt;-</span> <span class="fu">gen_fit2</span>(<span class="at">fit =</span> fit, <span class="at">rep =</span> <span class="dv">100</span>)</span></code></pre></div>
<p>The simulation can be run on multiple cores to speed up simulations.
After a while, the simulation is complete and the user can inspect the
results via the function flex_co2 providing the results and cutoffs for
selected fit indices under different decision rules as well as a plot of
the selected fit indices via plot_fit2. By default, CFI is used as the
index since it might be the easiest to understand. Indices can be
specified by argument index.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="fu">flex_co2</span>(fits)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="fu">plot_fit2</span>(fits)</span></code></pre></div>
<p>The main principle of this evaluation is that the user can decide
which cutoff is best for the given model, based on different acceptable
Type I errors (alpha), Type II errors (beta) and decision rules. The
tibble for evaluation is sorted in decreasing sum of error types. For
example, when the user only accepts an a-priori alpha and beta of .05,
the CP (cut point) decision rule leads to the lowest sum of error types
(Type I + II) for the given model with a cutoff of .956 for CFI (first
row of evaluation). The empirical CFI value however was .931
(fitmeasures, see above). While rejecting the model, the user commits an
error of .12 (.03 Type I, .09 Type II). When extending the simulation to
1,000 replications (rep = 1000L), the CP decision rule would commit an
error of .17 (sum of error types = .167), which indicates that model fit
is “close” since distributions somewhat overlap (11.1%).</p>
</div>
</div>
<div id="details" class="section level2">
<h2>Details</h2>
<div id="population-model" class="section level3">
<h3>Population model</h3>
<p>The basic idea of flexible cutoffs is that these cutoffs come from a
sui-generis (having it’s own shape) distribution of fit indices for a
correct, unbiased model. Essentially, this implies that the population
model is correctly specified with no errors. For example, all observed
variables load on the latent variables they are supposed to load on, all
correlations are estimated freely, and the data is not excessively
skewed. This assumption makes flexible cutoffs potentially more
objective since no subjective modification to the model
(misspecification) or data (skewness, kurtosis) is needed. However, this
also implies that the population model is specified not simply on the
basis of the own empirical model. In an anecdotal manner, one can
compare this to the introduction of the meter. Assuming that one’s own
measure is exactly 1 meter is likely error prone. One needs a validated
meter bar. The meter bar in this case is the population model.</p>
<p>The internal function pop_mod specifies this population model. We
offer three types of population models, the NM (Niemand &amp; Mai, 2018)
option, the HB (Hu &amp; Bentler, 1999) option, and the EM (empirical)
option. Function gen_fit (gen_fit2) internally calls function pop_mod,
but the argument type is also available in the latter function for
flexibility. We can compare these models by:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>FCO<span class="sc">:::</span><span class="fu">pop_mod</span>(<span class="at">mod =</span> HS.model, <span class="at">x =</span> lavaan<span class="sc">::</span>HolzingerSwineford1939)<span class="sc">$</span>pop.mod</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>FCO<span class="sc">:::</span><span class="fu">pop_mod</span>(<span class="at">mod =</span> HS.model, <span class="at">x =</span> lavaan<span class="sc">::</span>HolzingerSwineford1939, <span class="at">type =</span> <span class="st">&quot;HB&quot;</span>)<span class="sc">$</span>pop.mod</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>FCO<span class="sc">:::</span><span class="fu">pop_mod</span>(<span class="at">mod =</span> HS.model, <span class="at">x =</span> lavaan<span class="sc">::</span>HolzingerSwineford1939, <span class="at">type =</span> <span class="st">&quot;EM&quot;</span>)<span class="sc">$</span>pop.mod</span></code></pre></div>
<p>When the type is “NM”, all loadings (default: .7) and correlations
(default: .3) are assumed to be equal. When the type is “HB”, the
loadings vary by .05 around .75, depending on the number of indicators
and the correlations are either .5, .4, or .3, also depending on the
number of latent variables. Finally, when the type is “EM”, the function
runs a CFA and determines the empirical loadings and correlations. Since
one cannot assume the own empirical model to be correct, we advise users
to not use the “EM” type for model validation. This type might be useful
for other features (see further applications). Hence, the default is set
to “NM”. Since the by far most selected standardized factor loading
(afl) in our tool was .7, we set the default value to .7. Other options
between 0 and 1 are possible. The average correlation between latent
variables (aco) is set to a default of .3, but this can be changed
likewise. To increase flexibility, the argument standardized (default:
TRUE) can also be called, allowing for the specification of standardized
(all loadings &lt; 1, all covariances are correlations) and
unstandardized (loadings &gt; 1, covariances, not correlations)
parameters. The function returns a warning when the empirical model
suspects standardized or unstandardized loadings and this is in conflict
with the standardized argument.</p>
</div>
<div id="population-models-for-other-model-types" class="section level3">
<h3>Population models for other model types</h3>
<p>Starting with version 2.0, other model types other than CFA are also
possible, most prominently structural models via CBSEM (SEM). To
determine the correct population model, this however requires some
assumption, if the population type is “NM” or “HB” as one needs to know
what size loadings, correlations, regressions etc. should be assumed.
This is obviously not needed if type is “EM”, the default. For example,
let us assume the example model to be a SEM with a regression and no
correlation between F1 and F2:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>cmod <span class="ot">&lt;-</span> <span class="st">&quot;F1 =~ x1 + x2 + x3</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="st">          F2 =~ x4 + x5 + x6</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="st">          F3 =~ x7 + x8 + x9</span></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="st">          F3 ~ F1 + F2</span></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a><span class="st">          F1 ~~ .0 * F2&quot;</span></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">sem</span>(<span class="at">model =</span> cmod, <span class="at">data =</span> lavaan<span class="sc">::</span>HolzingerSwineford1939)</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a><span class="fu">fitmeasures</span>(fit2)</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>fits2 <span class="ot">&lt;-</span> <span class="fu">gen_fit2</span>(<span class="at">fit =</span> fit2, <span class="at">cfa =</span> <span class="cn">FALSE</span>, <span class="at">type =</span> <span class="st">&quot;NM&quot;</span>, <span class="at">es.f2 =</span> <span class="st">&quot;moderate&quot;</span>, <span class="at">rep =</span> <span class="dv">100</span>)</span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a><span class="fu">flex_co2</span>(fits2)</span></code></pre></div>
<p>By setting the (effect) sizes for loadings (es.lam), correlations
(es.cor) und regressions (es.f2) to “low”, “moderate” or “large”, users
can tailor these parameters to their assumptions, but still preserve the
same patterns of loadings and correlations as in population types “NM”
and “HB”. Size es.lam defaults to “low” (lambda = .7), “moderate” (.8)
and “large” (.9) are also possible. Based on Cohens (1988)
recommendations, es.cor, .1 (low), .3 (moderate) and .5 (large, default)
and es.f2, .02 (low), .15 (moderate, default) and .35 (large) are
available as options.</p>
<p>What this feature essentially does is avoiding an unfair comparison
of a SEM model (which usually has less perfect fit than a comparable CFA
as it might not incorporate all correlations, in the example: CFI =
.891) used to derive the fit indices with a CFA model (in the example:
CFI = .931) which was used to simulate the cutoffs. Instead, a SEM model
fit is compared with SEM-model based cutoffs. When extending the
simulation to 1,000 replications (rep = 1000L), the cutoffs would be
virtually certain that the model fit is “not acceptable” (sum of error
types = .02). As a side note, the internal function pop_mod_reg can be
used to inspect the population model, for example:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>FCO<span class="sc">:::</span><span class="fu">pop_mod_reg</span>(<span class="at">mod =</span> cmod, <span class="at">x =</span> lavaan<span class="sc">::</span>HolzingerSwineford1939, <span class="at">type =</span> <span class="st">&quot;NM&quot;</span>, <span class="at">es.f2 =</span> <span class="st">&quot;low&quot;</span>)<span class="sc">$</span>pop.mod</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>FCO<span class="sc">:::</span><span class="fu">pop_mod_reg</span>(<span class="at">mod =</span> cmod, <span class="at">x =</span> lavaan<span class="sc">::</span>HolzingerSwineford1939, <span class="at">type =</span> <span class="st">&quot;NM&quot;</span>, <span class="at">es.f2 =</span> <span class="st">&quot;moderate&quot;</span>)<span class="sc">$</span>pop.mod</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>FCO<span class="sc">:::</span><span class="fu">pop_mod_reg</span>(<span class="at">mod =</span> cmod, <span class="at">x =</span> lavaan<span class="sc">::</span>HolzingerSwineford1939, <span class="at">type =</span> <span class="st">&quot;NM&quot;</span>, <span class="at">es.f2 =</span> <span class="st">&quot;large&quot;</span>)<span class="sc">$</span>pop.mod</span></code></pre></div>
</div>
<div id="different-data-types" class="section level3">
<h3>Different data types</h3>
<p>Usually, multivariate normally distributed (normal) (manifest)
variables are assumed, which is often not the case. In the original
version, users could change, for example after a warning is given, the
arguments sk (skewness, default: 0) and ku (kurtosis, default: 1) in
gen_fit or gen_fit2 to account for multivariate non-normality. In this
case, the simulation function lavaan::simulateData produces data with
the set skewness and kurtosis. In version 2, the variable types can be
changed as well based on package PoisBinOrdNor, allowing for count,
binary, ordinal and normal variables. The user then only needs to
provide a vector data.types for each variable (in the proper order)
indicating “C” (count), “B” (binary), “O” (ordinal) and “N” (normal) as
an argument. For example:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>dat <span class="ot">&lt;-</span> lavaan<span class="sc">::</span>HolzingerSwineford1939</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>cdat <span class="ot">&lt;-</span> dplyr<span class="sc">::</span><span class="fu">select</span>(dat, x1, x2, x3, x4, x5, x6, x7, x8, x9)</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="co">#For demo purposes, some variables are changed:</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>cdat <span class="ot">&lt;-</span> cdat <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">round</span>(x1, <span class="at">digits =</span> <span class="dv">0</span>),</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>  <span class="at">x3 =</span> <span class="fu">round</span>(x3, <span class="at">digits =</span> <span class="dv">0</span>),</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">ifelse</span>(x2 <span class="sc">&gt;</span> <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>  <span class="at">x4 =</span> <span class="fu">ifelse</span>(x4 <span class="sc">&gt;</span> <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>  <span class="at">x5 =</span> <span class="fu">round</span>(x5, <span class="at">digits =</span> <span class="dv">0</span>),</span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>  <span class="at">x8 =</span> <span class="fu">round</span>(x8, <span class="at">digits =</span> <span class="dv">0</span>)</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a>)</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a>cfit <span class="ot">&lt;-</span> lavaan<span class="sc">::</span><span class="fu">cfa</span>(<span class="at">model =</span> HS.model, <span class="at">data =</span> cdat)</span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>cfits <span class="ot">&lt;-</span> <span class="fu">gen_fit2</span>(<span class="at">fit =</span> cfit,</span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a>                  <span class="at">data.types =</span> <span class="fu">c</span>(<span class="st">&quot;C&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;C&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;O&quot;</span>, <span class="st">&quot;N&quot;</span>, <span class="st">&quot;N&quot;</span>, <span class="st">&quot;O&quot;</span>, <span class="st">&quot;N&quot;</span>), <span class="at">rep =</span> <span class="dv">100</span>)</span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a><span class="fu">flex_co2</span>(cfits)</span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a><span class="fu">plot_fit2</span>(cfits)</span></code></pre></div>
<p>Internally, a function gen_nnd is called in this case, taking the
defined data types using the median as the lambda value (count), the
mean (binary, normal) or the frequencies of distinct values (ordinal) as
expected values from the data. Together with a Spearman-type correlation
matrix of the variables, the intermediate correlation matrix (see
PoisBinOrdNor::genPBONdata) is estimated, which is then used to
determine a new dataset used instead of the original data for simulating
the cutoffs.</p>
<p>For the example, deviating from normal variables leads to a worsened
fit (CFI = .902 instead of .931). Flexible cutoffs account for this data
and provide a fairer comparison compared to fixed cutoffs. When
extending the simulation to 1,000 replications (rep = 1000L), fixed
cutoffs would indicate an overall error of .343, compared to .266 for CP
and .277 for FCO1/2. That is, due to an still extensive overlap (15.3%),
the decision to reject the model is less error-prone when using flexible
cutoffs under these conditions.</p>
</div>
<div id="multi-core-support" class="section level3">
<h3>Multi-core support</h3>
<p>Since simulations need some time, we implemented multi-core support.
Depending on the system the package runs on, function gen_fit used
mclapply (on Linux or Mac) and parLapply (on Windows) from package
parallel for version &lt; 2. Gen_fit auto-detects the operating system
and chooses the proper function. Gen_fit2 uses foreach (on all
platforms) from package foreach for version 2. Due to the different
parallelization functions, results from gen_fit and gen_fit2 may be
slightly different.</p>
<p>In the common situation that users have multiple cores, more cores
can be set by the argument cores. However, note that the function
returns an error if the number of available cores of the system is lower
than the number of cores provided by the cores argument (e.g., cores =
4). Of course, multi-core support can be switched off by setting the
argument multi.core = FALSE (gen_fit) or cores = 1 (gen_fit2). For
example:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="fu">gen_fit2</span>(fit, <span class="at">cores =</span> <span class="dv">4</span>, <span class="at">rep =</span> <span class="dv">100</span>)</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="fu">gen_fit2</span>(fit, <span class="at">cores =</span> <span class="dv">1</span>, <span class="at">rep =</span> <span class="dv">100</span>)</span></code></pre></div>
</div>
<div id="decision-rules-version-2.0" class="section level3">
<h3>Decision rules (Version 2.0+)</h3>
<p>Instead of the recommendation function for older versions, version
2.0 offers a decision rule overview for the simulated fit values via the
function flex_co2. As explained in the overview chapter, multiple
decision rules are provided (FCO1, FCO2, DFI, CP, Fix). The function
flex_co2 returns a list of dataframes (tibbles) or characters. The first
one provides the quantiles for the provided arguments of alpha (a-priori
Type I error, default: .05) and beta (a-priori Type II error, default
.05 &amp; .10) for each fit index (default: CFI). The quantiles are
depending on the type of fit index (correct models and GoF: p, correct
models and BoF: 1-p, misfit models and GoF: 1-p, misfit models and BoF:
p) and are sorted by index and model type.</p>
<p>Second, a dataframe is provided that sorts all decision rules (column
“dec.rule”) by the sum of type I and II errors implied (column
“SumTypes”) in line with Hu &amp; Bentler (1999). So, the first row
represents the best decison rule and its cutoff (and fit index) in terms
of overall error. Some other statistics are also provided (e.g., Power,
Specificity). Third, a notation is provided assisting users to interpret
the evaluation. Finally, some important statistics regarding the
distributions (of each fit index) are returned, the percentage of
overlap between correct and misspecified models, the AUC (Area under the
curve) and a U-test (Effect size d). Percentage overlap indicates how
much the distributions of correct and misspecified models intersect each
other. A high percentage and a low AUC is indicative of a difficult
decision, i.e., the given misspecification is hard to detect.
Alternatively, when overlap percentage is low and AUC is high, the
cutoffs face a simple decision, i.e., the misspecification is easy to
detect. Finally, a U-test effect size (Cohens d converted from r)
provides a non-parametric estimate how distant the expected values from
both distributions are from each other. Again, a high value indicates a
small overlap, hence an easy decision for the cutoffs. This function
does not work with gen_fit.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co">#Default evaluation:</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="fu">flex_co2</span>(fits)</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="co">#Changed alpha and beta values:</span></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a><span class="fu">flex_co2</span>(fits, <span class="at">alpha =</span> .<span class="dv">05</span>, <span class="at">beta =</span> .<span class="dv">05</span>)</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a><span class="fu">flex_co2</span>(fits, <span class="at">alpha =</span> .<span class="dv">10</span>, <span class="at">beta =</span> .<span class="dv">20</span>)</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a><span class="co">#Different fit indices:</span></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a><span class="fu">flex_co2</span>(fits, <span class="at">index =</span> <span class="fu">c</span>(<span class="st">&quot;CFI&quot;</span>, <span class="st">&quot;SRMR&quot;</span>, <span class="st">&quot;RMSEA&quot;</span>))</span></code></pre></div>
</div>
<div id="plotting-version-2.0" class="section level3">
<h3>Plotting (Version 2.0+)</h3>
<p>Function plot_fit2 allows users to inspect the simulated
distributions of correct and misfit models based on the result from
gen_fit2 (e.g., fits). This function does not work with gen_fit. The
distribution of the correct models are depicted in darkblue, the misfit
models in orange. Vertical lines illustrate the quantiles for both based
on alpha (darkblue) and beta (orange) values and can be specified by
changing the alpha (default: .05) and beta (default: .10) arguments.
Indices can be specified by the argument index, which uses a facet.wrap
to display distributions for each index.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="co">#Default plot:</span></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="fu">plot_fit2</span>(fits)</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a><span class="co">#Changed alpha and beta values:</span></span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a><span class="fu">plot_fit2</span>(fits, <span class="at">alpha =</span> .<span class="dv">05</span>, <span class="at">beta =</span> .<span class="dv">05</span>)</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a><span class="fu">plot_fit2</span>(fits, <span class="at">alpha =</span> .<span class="dv">10</span>, <span class="at">beta =</span> .<span class="dv">20</span>)</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a><span class="co">#Different fit indices:</span></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a><span class="fu">plot_fit2</span>(fits, <span class="at">index =</span> <span class="fu">c</span>(<span class="st">&quot;CFI&quot;</span>, <span class="st">&quot;SRMR&quot;</span>, <span class="st">&quot;RMSEA&quot;</span>))</span></code></pre></div>
</div>
</div>
<div id="older-features-version-2" class="section level2">
<h2>Older features (Version &lt; 2)</h2>
<div id="recommendations-version-2" class="section level3">
<h3>Recommendations (Version &lt; 2)</h3>
<p>One quintessential issue we notice in many papers, reviews, and PhD
courses is that non-experts do not know which fit index or fit indicator
to choose. To summarize, this is the major question one of our papers
investigates (Mai et al., 2021) and the main message is that one should
follow a tailor-fit approach. Depending on three settings, a) sample
size, b) research purpose (novel or established model), and c) focus
(confirming a factorial structure, i.e., CFA or investigating a
theoretical, structural model), different fit indicators are
recommended.</p>
<p>The differentiation between novel or established model might need
some explanation. Fit indicators often yield different Type I and II
errors. When an established model that has been empirically investigated
many times before (e.g., Theory of Planned Behavior-models) is tested,
it makes sense to put more weight on Type I error. However, when the
model has never been tested before, it makes sense to put more weight on
the Type II error. Since fixed cutoffs show worse hit rates for higher
Type II error weights (1:3, 1:4), they may be poorly performing for
novel models.</p>
<p>We built the function recommend to incorporate this tailor-fit
approach in a user-friendly way. It requires the simulated fit indices
and the arguments for purpose and focus. Sample size is determined
automatically. Results are rounded to 3 digits, but can be changed to 1
to 5 digits if needed, for example by digits = 5.</p>
<p>Since the most obvious application is to conduct a CFA on a novel
model, the standard arguments of purpose and focus are set to this
application. Hence, when we use no further arguments, we get the
following recommendation:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="fu">data</span>(bb1992)</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="st">F1 =~ Q5 + Q7 + Q8</span></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a><span class="st">F2 =~ Q2 + Q4</span></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a><span class="st">F3 =~ Q10 + Q11 + Q12 + Q13 + Q18 + Q19 + Q20 + Q21 + Q22</span></span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a><span class="st">F4 =~ Q1 + Q17</span></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a><span class="st">F5 =~ Q6 + Q14 + Q15 + Q16</span></span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a><span class="st">&quot;</span></span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a>fits.single <span class="ot">&lt;-</span> <span class="fu">gen_fit</span>(<span class="at">mod1 =</span> mod, <span class="at">x =</span> bb1992, <span class="at">rep =</span> <span class="dv">100</span>)</span>
<span id="cb19-10"><a href="#cb19-10" tabindex="-1"></a><span class="fu">recommend</span>(fits.single)</span></code></pre></div>
<p>SRMR is recommended due to the purpose, focus, and sample size (n =
502) in line with the recommendations by Mai et al. (2021). Hence, the
lone fit indicator we need to report is SRMR. The function also returns
the type of the fit indicator, which is guessed from index_guess and the
actual value of the SRMR in the model.</p>
<p>Additionally, the function provides a sensitivity analysis for
different values of uncertainty, .001 (.1 percent), .01 (1 percent), .05
(5 percent) and .10 (10 percent) and makes a decision given the cutoff.
For completeness, replications, the number of non-converging models, and
their share are also provided.</p>
<p>The result found here demonstrates the consequences of uncertainty
for the decision. When one is very conservative and assumes a high Type
I error (.10), the cutoff (.036 for 100 replications) is lower than the
actual value (.038) and hence the present model should be rejected. When
one is very lenient and feels safe with the model (.001), the cutoff
(.040 for 100 replications) is higher than the actual value and hence
the model can be confirmed.</p>
<p>Let us assume for a moment that Babakus and Boller had not been as
exploratory as they were and simply looked for confirmation of an
established measurement model. So, we change the purpose argument to
established.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="fu">recommend</span>(fits.single, <span class="at">purpose =</span> <span class="st">&quot;established&quot;</span>)</span></code></pre></div>
<p>Now the recommendation changes to CFI with a fixed cutoff.
Consequently, no uncertainty is provided (as they are fixed) and the
recommendation is to confirm the model because the actual value (.960)
is above the fixed cutoff of .95. This demonstrates two things. First,
the recommend function also recommends fixed cutoffs when it is
acceptable to do so (see Mai et al. 2021). Second, it shows what happens
when assuming established models (and hence a low importance of Type II
error): Type I errors become more important. Compare this with the
lenient uncertainty before (.001, i.e., .040 for 100 replications), from
the first recommendation, where SRMR also confirms the model. In other
words, we feigned to be very certain by assuming the model to be an
established model (ignoring the doubts) and hence got a very determined
answer.</p>
<p>Finally, for exploratory investigations, one can also override the
recommendations programmed into the function by using the override
argument. This however requires users to provide one or more indices
with the argument index (otherwise, an error is returned).</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="fu">recommend</span>(fits.single, <span class="at">override =</span> <span class="cn">TRUE</span>, <span class="at">index =</span> <span class="fu">c</span>(<span class="st">&quot;CFI&quot;</span>, <span class="st">&quot;SRMR&quot;</span>))</span></code></pre></div>
<p>In the example, we provide CFI and SRMR and get the appropriate
recommendations for them. Unsurprisingly, the recommendations do not
change much (SRMR is confirmed for levels of .001 and .01 uncertainty
for 100 replications, otherwise the model is rejected). Please note that
purpose and focus are without function in this case, as the
recommendations by Mai et al. (2021) are overridden.</p>
</div>
<div id="relative-fit-comparison-version-2" class="section level3">
<h3>Relative fit comparison (Version &lt; 2)</h3>
<p>A popular feature in CBSEM is to compare nested models, for example
testing some type of invariance between groups, comparing alternative
theoretical models or discriminant validity testing (see next chapter
for the latter). So far, we allow users to incorporate a second model by
specifying the mod2 argument in function gen_fit. In this case, the
resulting fits from function are not vectors in a list of the length of
replications, but a matrix with two rows. Function flex_co then produces
a slightly different output displaying the flexible cutoffs for both
models as well as the difference between the two models.</p>
<p>Let us assume that the first two factors F1 and F2 might be
orthogonal (independent). Hence, we constrain the factor correlation
between both to zero (F1 ~~ 0 * F2).</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="fu">data</span>(bb1992)</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a><span class="st">F1 =~ Q5 + Q7 + Q8</span></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a><span class="st">F2 =~ Q2 + Q4</span></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a><span class="st">F3 =~ Q10 + Q11 + Q12 + Q13 + Q18 + Q19 + Q20 + Q21 + Q22</span></span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a><span class="st">F4 =~ Q1 + Q17</span></span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a><span class="st">F5 =~ Q6 + Q14 + Q15 + Q16</span></span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a><span class="st">&quot;</span></span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a>mod.con <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a><span class="st">F1 =~ Q5 + Q7 + Q8</span></span>
<span id="cb22-12"><a href="#cb22-12" tabindex="-1"></a><span class="st">F2 =~ Q2 + Q4</span></span>
<span id="cb22-13"><a href="#cb22-13" tabindex="-1"></a><span class="st">F3 =~ Q10 + Q11 + Q12 + Q13 + Q18 + Q19 + Q20 + Q21 + Q22</span></span>
<span id="cb22-14"><a href="#cb22-14" tabindex="-1"></a><span class="st">F4 =~ Q1 + Q17</span></span>
<span id="cb22-15"><a href="#cb22-15" tabindex="-1"></a><span class="st">F5 =~ Q6 + Q14 + Q15 + Q16</span></span>
<span id="cb22-16"><a href="#cb22-16" tabindex="-1"></a><span class="st">F1 ~~ 0 * F2</span></span>
<span id="cb22-17"><a href="#cb22-17" tabindex="-1"></a><span class="st">&quot;</span> </span>
<span id="cb22-18"><a href="#cb22-18" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" tabindex="-1"></a>fits.con <span class="ot">&lt;-</span> <span class="fu">gen_fit</span>(<span class="at">mod1 =</span> mod, <span class="at">mod2 =</span> mod.con, <span class="at">x =</span> bb1992, <span class="at">rep =</span> <span class="dv">100</span>) </span>
<span id="cb22-20"><a href="#cb22-20" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" tabindex="-1"></a><span class="fu">flex_co</span>(<span class="at">fits =</span> fits.con, <span class="at">index =</span> <span class="fu">c</span>(<span class="st">&quot;CFI&quot;</span>, <span class="st">&quot;SRMR&quot;</span>), <span class="at">alpha.lev =</span> .<span class="dv">05</span>) </span>
<span id="cb22-22"><a href="#cb22-22" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" tabindex="-1"></a><span class="fu">fitmeasures</span>(<span class="fu">cfa</span>(<span class="at">model =</span> mod, <span class="at">data =</span> bb1992), <span class="at">fit.measures =</span> <span class="fu">c</span>(<span class="st">&quot;cfi&quot;</span>, <span class="st">&quot;srmr&quot;</span>)) <span class="sc">-</span> <span class="fu">fitmeasures</span>(<span class="fu">cfa</span>(<span class="at">model =</span> mod.con, <span class="at">data =</span> bb1992), <span class="at">fit.measures =</span> <span class="fu">c</span>(<span class="st">&quot;cfi&quot;</span>, <span class="st">&quot;srmr&quot;</span>))</span></code></pre></div>
<p>Since the correlation between both factors was only .36, it is likely
that the models show a comparable fit. Indeed, the flexible cutoffs are
slightly worsened in the constrained model 2 (CFI = .956, SRMR = .056)
compared to model 1 (CFI = .969, SRMR = .035), yielding a small
difference (delta CFI = .012, delta SRMR = -.022). Consider that this
difference is larger than the present difference of CFI between the
models (.011) and smaller than present difference of SRMR between the
models (-.037). That is, CFI was - as expected - less sensitive to the
constraint than SRMR. Put simply, flexible cutoffs suggest that the
change in fit by CFI is acceptable (flexible cutoff: .012 &gt; present
difference: .011), but the change in fit by SRMR is not (flexible
cutoff: -.020 &gt; present difference: -.037). We should reject the
alternative model of orthogonal factors F1 and F2. Fixed cutoffs (CFI ≥
.95, SRMR ≤ .09) would still have accepted the alternative model. Please
note that guidelines for relative fit comparisons in a contingent way -
accounting for model size and sample size - are rare to find (e.g.,
Meade et al. 2008 for measurement invariance). Future investigations of
the performance of flexible cutoffs for relative fit comparisons might
be interesting.</p>
<p>As a technical side note, the flexible cutoffs here are slightly
different from the ones described in chapter “Basic usage” since the
type argument is automatically switched to “EM” (type = “EM”) when two
models are provided. When single cutoffs would be generated with this
setting, the same flexible cutoffs would be found.</p>
</div>
<div id="discriminant-validity-testing-version-2" class="section level3">
<h3>Discriminant validity testing (Version &lt; 2)</h3>
<p>A special application of flexible cutoffs could be its ability to
test discriminant validity. As Rönkkö &amp; Cho (2022) highlighted,
CBSEM could be useful to compare CFAs where a second alternative model
merges, hereafter “merging”, two factors subject to an issue in
discriminant validity (given a substantially high correlation between
the factors). Alternatively, it is possible to compare an unconstrained
model with a constrained model, where the correlation between the
factors is set to a cutoff value (e.g., .9). We refer to this second
principle as “constraining”. Instead of testing the chi-square
difference between the original and the alternative model, flexible
cutoffs for fit indicators might work equally well or even better, given
the issues with the chi-square statistic (e.g., Niemand &amp; Mai
2018).</p>
<p>Consider that in the example data, factors F4 and F5 are highly
correlated (.831), possibly indicating an issue in discriminant
validity. To investigate this, we can use the discriminant
validity-related arguments of the gen_fit function, termed dv,
dv.factors, merge.mod, and dv.cutoff. Argument dv simply tells the
function to expect the application of discriminant validity testing.
Argument dv.factors provides the factors that should be investigated, in
this case F4 and F5. If this argument is missing, it is assumed that the
first and second factor of the model should be investigated (F1 &amp;
F2). Hence, one should provide the names of the factors if that is not
the case. Argument merge.mod is needed if merging should be applied.
This argument calls an internal function that takes the original model,
changes all indicators of the second factor (F5) specified under
dv.factors to be indicators of the first factor (F4), and omits the
second factor from estimation. Please note that it does not matter which
factor is first or second, as both are merged anyway. Finally, dv.cutoff
is required when one wants to apply constraining. The value for this
cutoff can be between 0 and 1, but a warning is given if it is lower
than .8, as this is generally accepted as the lower border of a
discriminant validity issue (Rönkkö &amp; Cho 2022). Some guessing work
is implemented here to make the arguments more convenient. For example,
if one forgot the argument dv but sets merge.mod = TRUE, merging is
still assumed. To make sure the user knows which approach is used, a
message indicating the discriminant validity testing approach is
returned.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="fu">data</span>(bb1992)</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a><span class="st">F1 =~ Q5 + Q7 + Q8</span></span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a><span class="st">F2 =~ Q2 + Q4</span></span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a><span class="st">F3 =~ Q10 + Q11 + Q12 + Q13 + Q18 + Q19 + Q20 + Q21 + Q22</span></span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a><span class="st">F4 =~ Q1 + Q17</span></span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a><span class="st">F5 =~ Q6 + Q14 + Q15 + Q16</span></span>
<span id="cb23-8"><a href="#cb23-8" tabindex="-1"></a><span class="st">&quot;</span></span>
<span id="cb23-9"><a href="#cb23-9" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" tabindex="-1"></a>fits.dv.con <span class="ot">&lt;-</span> <span class="fu">gen_fit</span>(<span class="at">mod1 =</span> mod, <span class="at">x =</span> bb1992, <span class="at">rep =</span> <span class="dv">100</span>, <span class="at">dv =</span> <span class="cn">TRUE</span>, <span class="at">dv.factors =</span> <span class="fu">c</span>(<span class="st">&quot;F4&quot;</span>, <span class="st">&quot;F5&quot;</span>), <span class="at">dv.cutoff =</span> .<span class="dv">9</span>) </span>
<span id="cb23-11"><a href="#cb23-11" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" tabindex="-1"></a>fits.dv.merge <span class="ot">&lt;-</span> <span class="fu">gen_fit</span>(<span class="at">mod1 =</span> mod, <span class="at">x =</span> bb1992, <span class="at">rep =</span> <span class="dv">100</span>, <span class="at">dv =</span> <span class="cn">TRUE</span>, <span class="at">dv.factors =</span> <span class="fu">c</span>(<span class="st">&quot;F4&quot;</span>, <span class="st">&quot;F5&quot;</span>), <span class="at">merge.mod =</span> <span class="cn">TRUE</span>)</span>
<span id="cb23-13"><a href="#cb23-13" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" tabindex="-1"></a><span class="fu">flex_co</span>(<span class="at">fits =</span> fits.dv.con, <span class="at">index =</span> <span class="st">&quot;CFI&quot;</span>, <span class="at">alpha.lev =</span> .<span class="dv">05</span>) </span>
<span id="cb23-15"><a href="#cb23-15" tabindex="-1"></a><span class="fu">flex_co</span>(<span class="at">fits =</span> fits.dv.merge, <span class="at">index =</span> <span class="st">&quot;CFI&quot;</span>, <span class="at">alpha.lev =</span> .<span class="dv">05</span>)</span>
<span id="cb23-16"><a href="#cb23-16" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" tabindex="-1"></a>mod.dv.con <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb23-18"><a href="#cb23-18" tabindex="-1"></a><span class="st">F1 =~ Q5 + Q7 + Q8</span></span>
<span id="cb23-19"><a href="#cb23-19" tabindex="-1"></a><span class="st">F2 =~ Q2 + Q4</span></span>
<span id="cb23-20"><a href="#cb23-20" tabindex="-1"></a><span class="st">F3 =~ Q10 + Q11 + Q12 + Q13 + Q18 + Q19 + Q20 + Q21 + Q22</span></span>
<span id="cb23-21"><a href="#cb23-21" tabindex="-1"></a><span class="st">F4 =~ Q1 + Q17</span></span>
<span id="cb23-22"><a href="#cb23-22" tabindex="-1"></a><span class="st">F5 =~ Q6 + Q14 + Q15 + Q16</span></span>
<span id="cb23-23"><a href="#cb23-23" tabindex="-1"></a><span class="st">F4 ~~ .9 * F5</span></span>
<span id="cb23-24"><a href="#cb23-24" tabindex="-1"></a><span class="st">&quot;</span> </span>
<span id="cb23-25"><a href="#cb23-25" tabindex="-1"></a></span>
<span id="cb23-26"><a href="#cb23-26" tabindex="-1"></a><span class="fu">fitmeasures</span>(<span class="fu">cfa</span>(<span class="at">model =</span> mod.dv.con, <span class="at">data =</span> bb1992, <span class="at">auto.fix.first =</span> <span class="cn">FALSE</span>, <span class="at">std.lv =</span> <span class="cn">TRUE</span>), <span class="at">fit.measures =</span> <span class="st">&quot;cfi&quot;</span>)</span>
<span id="cb23-27"><a href="#cb23-27" tabindex="-1"></a></span>
<span id="cb23-28"><a href="#cb23-28" tabindex="-1"></a>mod.dv.merge <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb23-29"><a href="#cb23-29" tabindex="-1"></a><span class="st">F1 =~ Q5 + Q7 + Q8</span></span>
<span id="cb23-30"><a href="#cb23-30" tabindex="-1"></a><span class="st">F2 =~ Q2 + Q4</span></span>
<span id="cb23-31"><a href="#cb23-31" tabindex="-1"></a><span class="st">F3 =~ Q10 + Q11 + Q12 + Q13 + Q18 + Q19 + Q20 + Q21 + Q22</span></span>
<span id="cb23-32"><a href="#cb23-32" tabindex="-1"></a><span class="st">F4 =~ Q1 + Q17 + Q6 + Q14 + Q15 + Q16</span></span>
<span id="cb23-33"><a href="#cb23-33" tabindex="-1"></a><span class="st">&quot;</span> </span>
<span id="cb23-34"><a href="#cb23-34" tabindex="-1"></a></span>
<span id="cb23-35"><a href="#cb23-35" tabindex="-1"></a><span class="fu">fitmeasures</span>(<span class="fu">cfa</span>(<span class="at">model =</span> mod.dv.merge, <span class="at">data =</span> bb1992), <span class="at">fit.measures =</span> <span class="st">&quot;cfi&quot;</span>)</span></code></pre></div>
<p>In this example, we asked for discriminant validity testing (dv =
TRUE) and ran constraining first, constraining the correlation between
the provided factors (dv.factors = c(“F4”, “F5”)) to the cutoff of .9
(dv.cutoff = .9). Second, we ran merging, this time telling the function
to merge both selected factors (merge.mod = TRUE).</p>
<p>Similar to the relative fit comparison case, the function returns
tables for each replication, which then can be handled by the flex_co
function. There are multiple options for how to assess discriminant
validity subject to further investigations. As a simple test, we apply
the following logic here (essentially a variant of a chi-squared
difference test only with CFI): If two factors are clearly discriminant,
their correlation is low, yielding a worse fit for the constrained or
merged model compared to the original (unconstrained, not merged) model.
This implies that the a GoF index, such as CFI, is smaller than the
cutoff of the correct model (e.g., .75 &lt; .95). In contrast, a BoF
index, such as SRMR, would indicate discriminant validity if it is
larger than the respective cutoff of the correct model (e.g., .15 &gt;
.05). In this example, the CFI for constraining is .959, while cutoffs
for the correct model (model 1) are .969. In a nutshell, the CFI of the
constrained model is out of the range of simulated CFIs for correct
models (in this case, outside of 95% of all CFIs simulated). For
merging, the same picture is found. CFI is .952, which is well below the
cutoff of .969. Multiple other options are plausible, but we leave the
point here.</p>
<p>Since the code to obtain these values is rather extensive and hence
to make user’s work easier, we provide a recommendation function
recommend_dv that incorporates the previously described considerations.
As with the recommend function for single fit indicators, it requires a
result from gen_fit with appropriate arguments and, optionally, names of
the fit indices (CFI is used if not provided). Results are rounded to 3
digits, but can be changed to 1 to 5 digits if needed, for example by
digits = 5.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="fu">recommend_dv</span>(fits.dv.con) </span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a><span class="fu">recommend_dv</span>(fits.dv.merge)</span></code></pre></div>
<p>The function returns the cutoffs for different levels of alpha and
the two models, where the first model is the original model and the
second model is termed constrained or merged based on the approach
selected in the simulation. Further, the actual fit values are
automatically provided. Hence, the constrained or merged model does not
needed to be defined explicitly. Differences and decisions based on the
different alpha values are provided as well. Finally, the number of
replications as well as a comment highlighting the approach and an
interpretation of the decision basis (the simple test) are given. Please
note that the decisions are identical to the ones made by Rönkkö &amp;
Cho’s tool in semTools::discriminantValidity.</p>
<p>## References</p>
<p>Babakus, E., &amp; Boller, G. W. (1992). An empirical assessment of
the SERVQUAL scale. Journal of Business Research, 24(3), 253–268. <a href="https://doi.org/10.1016/0148-2963(92)90022-4" class="uri">https://doi.org/10.1016/0148-2963(92)90022-4</a></p>
<p>Cohen, J. (1988). Statistical power analysis for the behavioral
sciences. Lawrence Erlbaum. <a href="https://doi.org/10.4324/9780203771587" class="uri">https://doi.org/10.4324/9780203771587</a></p>
<p>Groskurth, K., Bhaktha, N., &amp; Lechner, C. M. (2025). The
simulation-cum-ROC approach: A new approach to generate tailored cutoffs
for fit indices through simulation and ROC analysis. Behavior Research
Methods, 57(5), Article 135. <a href="https://doi.org/10.3758/s13428-025-02638-x" class="uri">https://doi.org/10.3758/s13428-025-02638-x</a></p>
<p>Hayes, A. F. (2017). Introduction to mediation, moderation, and
conditional process analysis: A regression-based approach (2nd ed.). The
Guilford Press.</p>
<p>Hu, L., &amp; Bentler, P. M. (1999). Cutoff criteria for fit indexes
in covariance structure analysis: Conventional criteria versus new
alternatives. Structural Equation Modeling, 6(1), 1–55. <a href="https://doi.org/10.1080/10705519909540118" class="uri">https://doi.org/10.1080/10705519909540118</a></p>
<p>Mai, R., Niemand, T., &amp; Kraus, S. (2021). A Tailor-Fit Model
Evaluation Strategy for Better Decisions about Structural Equation
Models. Technological Forecasting &amp; Social Change, 173(December)
121142. <a href="https://doi.org/10.1016/j.techfore.2021.121142" class="uri">https://doi.org/10.1016/j.techfore.2021.121142</a></p>
<p>McNeish, D., &amp; Wolf, M. G. (2023). Dynamic fit index cutoffs for
confirmatory factor analysis models. Psychological Methods, 28(1),
61–88. <a href="https://doi.org/10.1037/met0000425" class="uri">https://doi.org/10.1037/met0000425</a></p>
<p>Meade, A. W., Johnson, E. C., &amp; Braddy, P. W. (2008). Power and
sensitivity of alternative fit indices in tests of measurement
invariance. Journal of Applied Psychology, 93(3), 568-592. <a href="https://doi.org/10.1037/0021-9010.93.3.568" class="uri">https://doi.org/10.1037/0021-9010.93.3.568</a></p>
<p>Niemand, T., &amp; Mai, R. (2018). Flexible cutoff values for fit
indices in the evaluation of structural equation models. Journal of the
Academy of Marketing Science, 46(6), 1148—1172. <a href="https://doi.org/10.1007/s11747-018-0602-9" class="uri">https://doi.org/10.1007/s11747-018-0602-9</a></p>
<p>Rönkkö, M., &amp; Cho, E. (2022). An Updated Guideline for Assessing
Discriminant Validity. Organizational Research Methods, 25(1), 6–14. <a href="https://doi.org/10.1177/1094428120968614" class="uri">https://doi.org/10.1177/1094428120968614</a></p>
<p>Comment: V09052025</p>
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
